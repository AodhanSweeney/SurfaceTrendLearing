{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(35, 270) (35, 930)\n",
      "(35, 270) (35, 60)\n",
      "(35, 270) (35, 60)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3653468/2040182248.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;31m# Get the principle components from the training data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m     \u001b[0meofs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpcs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meigenvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meof_and_pc_finder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTrueTrendsTrain_structure_X_sample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m     \u001b[0mN_important\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m35\u001b[0m \u001b[0;31m# choosen to match the amount of PCs needed to represent 80% of variability\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_3653468/2040182248.py\u001b[0m in \u001b[0;36meof_and_pc_finder\u001b[0;34m(mat_structure_X_sample)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mQuestion\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mMu\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mTing\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mShould\u001b[0m \u001b[0mwe\u001b[0m \u001b[0mbe\u001b[0m \u001b[0musing\u001b[0m \u001b[0meofs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mcorrected\u001b[0m \u001b[0mamplitude\u001b[0m \u001b[0meofs\u001b[0m\u001b[0;31m?\u001b[0m\u001b[0;31m?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \"\"\"\n\u001b[0;32m---> 60\u001b[0;31m     \u001b[0mu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msvd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmat_structure_X_sample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfull_matrices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m     \u001b[0meofs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0msmat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10368\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m270\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcomplex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36msvd\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/numpy/linalg/linalg.py\u001b[0m in \u001b[0;36msvd\u001b[0;34m(a, full_matrices, compute_uv, hermitian)\u001b[0m\n\u001b[1;32m   1658\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1659\u001b[0m         \u001b[0msignature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'D->DdD'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misComplexType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'd->ddd'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1660\u001b[0;31m         \u001b[0mu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgufunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msignature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1661\u001b[0m         \u001b[0mu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1662\u001b[0m         \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_realType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# import statements\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import glob\n",
    "import random\n",
    "from sklearn import decomposition\n",
    "\n",
    "def train_test_splitting(xarray_file):\n",
    "    \"\"\"\n",
    "    Split model data into train and testing. 10 ensemble \n",
    "    members for each model are used for training as to \n",
    "    not over weight a given model. All remaining ensemble \n",
    "    members are used for testing.\n",
    "    \"\"\"\n",
    "    # Find number of ensembles and random indicies of testing versus training ensembels\n",
    "    Nensembles = len(xarray_file.ensemble_member)\n",
    "    ensemble_train_indices = random.sample(range(0,Nensembles),9)\n",
    "    ensemble_test_indices = list(set(list(range(0,Nensembles))).difference(ensemble_train_indices))\n",
    "\n",
    "    # Select natural and forced trends as well as the training and testing data\n",
    "    trend_data = xarray_file.to_array()[0]\n",
    "    NatTrendsTrain = trend_data[ensemble_train_indices,0].to_numpy()\n",
    "    NatTrendsTest = trend_data[ensemble_test_indices,0].to_numpy()\n",
    "    ForTrendsTrain = trend_data[ensemble_train_indices,1].to_numpy()\n",
    "    ForTrendsTest = trend_data[ensemble_test_indices,1].to_numpy()\n",
    "\n",
    "    return(NatTrendsTrain, NatTrendsTest, ForTrendsTrain, ForTrendsTest)\n",
    "\n",
    "def model_ensemble_reshaper(trends):\n",
    "    \"\"\"\n",
    "    Takes a given models testing or trainging data and \n",
    "    reshapes it so that timeperiods from different ensembles\n",
    "    of a given model are treated equally.\n",
    "    \"\"\"\n",
    "    reshaped_trends = np.reshape(trends, (np.shape(trends)[0]*np.shape(trends)[1], 72,144))\n",
    "    return(reshaped_trends)\n",
    "\n",
    "def predictor_reshaper(trends):\n",
    "    \"\"\"\n",
    "    Takes maps of trends and reshapes grid points into a vector.\n",
    "    \"\"\"\n",
    "    PredictorVector = np.reshape(trends, (np.shape(trends)[0], np.shape(trends)[1]*np.shape(trends)[2]))\n",
    "    return(PredictorVector)\n",
    "\n",
    "def tropical_mean_trend(trends):\n",
    "    \"\"\" \n",
    "    Takes map of trends and finds average over the 30S-30N region.\n",
    "    \"\"\"\n",
    "    ReshapedTrends = np.reshape(trends[:,24:48,:], (np.shape(trends)[0],24*144))\n",
    "    TropicalAverageTrend = np.average(ReshapedTrends, axis=1)\n",
    "    return(TropicalAverageTrend)\n",
    "\n",
    "def eof_and_pc_finder(mat_structure_X_sample):\n",
    "    \"\"\"\n",
    "    Question for Mu-Ting: Should we be using eofs or the corrected amplitude eofs??\n",
    "    \"\"\"\n",
    "    u, s, vh = np.linalg.svd(mat_structure_X_sample, full_matrices=True)\n",
    "    eofs = u.T\n",
    "    smat = np.zeros((10368, 270), dtype=complex)\n",
    "    smat[:270, :270] = np.diag(s)\n",
    "\n",
    "    #get eigenvalues\n",
    "    eigenvalues = (np.multiply(s,s))/len(s)\n",
    "    eigenvalues_as_frac_of_cov = eigenvalues/(np.sum(eigenvalues))\n",
    "\n",
    "    #create eofs with correct amplitude\n",
    "    #D = np.matmul(u, smat)/(np.sqrt(270))\n",
    "    #corrected_amplitude_eof = D.T\n",
    "    #find Principal Components\n",
    "    pcs = np.matmul(smat, vh).real\n",
    "\n",
    "    return(eofs, pcs, eigenvalues_as_frac_of_cov)\n",
    "\n",
    "\n",
    "path_to_data = '/home/disk/pna2/aodhan/SurfaceTrendLearning/*.nc'\n",
    "ModelDataFiles = glob.glob(path_to_data)\n",
    "\n",
    "TraingPredictorData = []\n",
    "TrainingTargetData = []\n",
    "TestingPredictorData = []\n",
    "TestingTargetData = []\n",
    "for datafile in ModelDataFiles:\n",
    "    xarray_file = xr.open_dataset(datafile) \n",
    "    # find training and testing data for natural and forced trends\n",
    "    NatTrendsTrain, NatTrendsTest, ForTrendsTrain, ForTrendsTest = train_test_splitting(xarray_file)\n",
    "\n",
    "    # reshape trends so that trend maps from different time periods and ensembles are treated equal\n",
    "    NatTrendsTrain = model_ensemble_reshaper(NatTrendsTrain)\n",
    "    NatTrendsTest = model_ensemble_reshaper(NatTrendsTest)\n",
    "    ForTrendsTrain = model_ensemble_reshaper(ForTrendsTrain)\n",
    "    ForTrendsTest = model_ensemble_reshaper(ForTrendsTest)\n",
    "\n",
    "    # weight trend maps by cosine of latitude\n",
    "    weights = np.cos(np.deg2rad(xarray_file.Lat.to_numpy())) # these will be used to weight predictors\n",
    "    NatTrendsTrain_weighted = np.multiply(NatTrendsTrain, weights[np.newaxis,:,np.newaxis])\n",
    "    NatTrendsTest_weighted = np.multiply(NatTrendsTest, weights[np.newaxis,:,np.newaxis])\n",
    "    ForTrendsTrain_weighted = np.multiply(ForTrendsTrain, weights[np.newaxis,:,np.newaxis])\n",
    "    ForTrendsTest_weighted = np.multiply(ForTrendsTest, weights[np.newaxis,:,np.newaxis])\n",
    "    \n",
    "    # true trend maps are sum of natural and forced trends\n",
    "    # find true trends for training data\n",
    "    TrueTrendsTrain = NatTrendsTrain_weighted + ForTrendsTrain_weighted\n",
    "    TrainShape = np.shape(TrueTrendsTrain)\n",
    "    TrueTrendsTrain_sample_X_structure = np.reshape(TrueTrendsTrain, (TrainShape[0], TrainShape[1]*TrainShape[2]))\n",
    "    TrueTrendsTrain_structure_X_sample = TrueTrendsTrain_sample_X_structure.T\n",
    "\n",
    "    # find true trends for testing data\n",
    "    TrueTrendsTest = NatTrendsTest_weighted + ForTrendsTest_weighted\n",
    "    TestShape = np.shape(TrueTrendsTest)\n",
    "    TrueTrendsTest_sample_X_structure = np.reshape(TrueTrendsTest, (TestShape[0], TestShape[1]*TestShape[2]))\n",
    "    TrueTrendsTest_structure_X_sample = TrueTrendsTest_sample_X_structure.T\n",
    "\n",
    "    # Get the principle components from the training data\n",
    "    eofs, pcs, eigenvalues = eof_and_pc_finder(TrueTrendsTrain_structure_X_sample)\n",
    "    N_important = 35 # choosen to match the amount of PCs needed to represent 80% of variability\n",
    "\n",
    "    # ImportantPrincipleComponents is a vector with 35 coefficients of eofs (pcs) for each of the 270 training trends\n",
    "    ImportantPrincipleComponents = pcs[:N_important] \n",
    "    ImportantEOFS = eofs[:N_important]\n",
    "    # project maps onto the eofs\n",
    "    ImportantEOFS = eofs[:N_important]\n",
    "    TrainingFeatures = ImportantEOFS@TrueTrendsTrain_structure_X_sample\n",
    "    TestingFeatures = ImportantEOFS@TrueTrendsTest_structure_X_sample\n",
    "\n",
    "    print(np.shape(TrainingFeatures), np.shape(TestingFeatures))\n",
    "\n",
    "    # reshape predictors as vector\n",
    "    TrainingTrends_vectors = TrainingFeatures.T#predictor_reshaper(TrueTrendsTrain)\n",
    "    TestingTrends_vectors = TestingFeatures.T#predictor_reshaper(TrueTrendsTest)\n",
    "\n",
    "    # find tropical mean trend value\n",
    "    NatTrendsTrainTropicalMean = tropical_mean_trend(NatTrendsTrain_weighted)\n",
    "    NatTrendsTestTropicalMean = tropical_mean_trend(NatTrendsTest_weighted)\n",
    "    ForTrendsTrainTropicalMean = tropical_mean_trend(ForTrendsTrain_weighted)\n",
    "    ForTrendsTestTropicalMean = tropical_mean_trend(ForTrendsTest_weighted)\n",
    "\n",
    "    [TraingPredictorData.append(TrainingTrends_vectors[i]) for i in range(len(TrainingTrends_vectors))]\n",
    "    [TrainingTargetData.append([NatTrendsTrainTropicalMean[i], ForTrendsTrainTropicalMean[i]]) \n",
    "    for i in range(len(ForTrendsTrainTropicalMean))]\n",
    "    [TestingPredictorData.append(TestingTrends_vectors[i]) for i in range(len(TestingTrends_vectors))]\n",
    "    [TestingTargetData.append([NatTrendsTestTropicalMean[i], ForTrendsTestTropicalMean[i]]) \n",
    "    for i in range(len(ForTrendsTestTropicalMean))]\n",
    "\n",
    "TraingPredictorData = np.array(TraingPredictorData)\n",
    "TrainingTargetData = np.array(TrainingTargetData)\n",
    "TestingPredictorData = np.array(TestingPredictorData)\n",
    "TestingTargetData = np.array(TestingTargetData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## It may make sense to find PCs for just the natural variability, and then traiin the model on that"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "af835c7fa3f1d66f76e7463e82810696e6cb13c598b94db10ac6143493e4a761"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
