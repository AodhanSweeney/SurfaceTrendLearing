{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-06 16:12:51.963719: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# import statements\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import glob\n",
    "import PolarTestingTrainingSplit_CV\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import keras_tuner\n",
    "from keras_tuner import BayesianOptimization\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from numpy.random import seed\n",
    "seed(0)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(0)\n",
    "\n",
    "# Get names of models in which we are testing on\n",
    "path_to_data = '/home/disk/pna2/aodhan/SurfaceTrendLearning/PoChedleyEtAl2022/TASmaps/*_TrendMaps.nc'\n",
    "ModelNames = [i[70:-16] for i in glob.glob(path_to_data)]\n",
    "\n",
    "# Do train-test-split \n",
    "TrainingPredictorData, TrainingTargetData, TestingPredictorData, TestingTargetData, TestingTotalTrend = PolarTestingTrainingSplit_CV.training_testing_split(path_to_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-06 16:15:31.255301: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-06 16:15:31.304376: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Oracle triggered exit\n",
      "ACCESS_ESM1_5_\n",
      "MSE: 0.0007407377706840634\n",
      "Optimal HPs:  12 0.0001\n",
      "\n",
      "\n",
      "__________________________________________________\n",
      "INFO:tensorflow:Oracle triggered exit\n",
      "CESM2_\n",
      "MSE: 0.00031731926719658077\n",
      "Optimal HPs:  8 0.0001\n",
      "\n",
      "\n",
      "__________________________________________________\n",
      "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.decay\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.learning_rate\n"
     ]
    }
   ],
   "source": [
    "# Store summaries\n",
    "summaries = []\n",
    "\n",
    "# Define the model\n",
    "def build_model(hp):\n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.layers.Dense(units=hp.Int('units', min_value=2, max_value=24, step=2), activation='relu', input_shape=(6,)))\n",
    "    model.add(keras.layers.Dense(units=1))\n",
    "    model.compile(optimizer=keras.optimizers.Adam(hp.Choice('learning_rate', [1e-4,1e-5])), loss='mean_squared_error')\n",
    "    return model\n",
    "\n",
    "for model_idx in range(0, len(ModelNames)):\n",
    "    # Define the tuner\n",
    "    tuner = BayesianOptimization(build_model, objective='val_loss', max_trials=8, overwrite=True, \n",
    "    directory='/home/disk/p/aodhan/SurfaceTrendLearing/TropicalTASApplication/output',\n",
    "    project_name=\"NN_trial_{model}\".format(model=ModelNames[model_idx]), seed=0)\n",
    "    stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
    "\n",
    "    # find specific model cross validation data\n",
    "    TrainingTargetDataShape = np.shape(TrainingTargetData[model_idx])\n",
    "    TestinTargetDataShape = np.shape(TestingTargetData[model_idx])\n",
    "    TrainingTargetDataReshaped = np.reshape(TrainingTargetData[model_idx], (TrainingTargetDataShape[0], TrainingTargetDataShape[1]*TrainingTargetDataShape[2]))\n",
    "    TestingTargetDataReshaped = np.reshape(TestingTargetData[model_idx], (TestinTargetDataShape[0], TestinTargetDataShape[1]*TestinTargetDataShape[2]))\n",
    "    TrainingTargetDataReshaped = np.transpose(TrainingTargetDataReshaped[:,0])\n",
    "    TestingTargetDataReshaped = np.transpose(TestingTargetDataReshaped[:,0])\n",
    "\n",
    "    # Model Design\n",
    "    pls = PLSRegression(n_components=6)\n",
    "    \n",
    "    # Use PLS regression to find reduced space\n",
    "    pls_model = pls.fit(TrainingPredictorData[model_idx], TrainingTargetDataReshaped)\n",
    "    X_train_pls = pls.transform(TrainingPredictorData[model_idx])\n",
    "    X_test_pls = pls.transform(TestingPredictorData[model_idx])\n",
    "    \n",
    "    # Set training and testing data\n",
    "    X_train =X_train_pls# np.reshape(TrainingPredictorData[model_idx], (len(TrainingPredictorData[model_idx]),72*144))\n",
    "    X_test = X_test_pls#np.reshape(TestingPredictorData[model_idx], (len(TestingPredictorData[model_idx]),72*144))\n",
    "    Y_train = TrainingTargetDataReshaped\n",
    "    Y_test = TestingTargetDataReshaped\n",
    "\n",
    "    # Fit the model using separate training and validation data\n",
    "    history = tuner.search(X_train, Y_train, epochs=250, validation_data=(X_test, Y_test), \n",
    "                           verbose=0, callbacks=[stop_early])\n",
    "\n",
    "    # Get best model\n",
    "    best_model = tuner.get_best_models()[0]\n",
    "    \n",
    "    # evaluate best model\n",
    "    val_loss = best_model.evaluate(X_test, Y_test, verbose=0)\n",
    "\n",
    "    # save best hyper parameters and save them\n",
    "    best_hps = tuner.get_best_hyperparameters()[0]\n",
    "    summaries.append([ModelNames[model_idx], best_hps.get('units'), best_hps.get('learning_rate'), val_loss])\n",
    "    \n",
    "    # Print preformance and optimal hyper parameters\n",
    "    print(ModelNames[model_idx])\n",
    "    print('MSE:', val_loss)\n",
    "    print('Optimal HPs: ', best_hps.get('units'), best_hps.get('learning_rate'))\n",
    "    print('__________________________________________________')\n",
    "\n",
    "print(summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['ACCESS_ESM1_5_' '6' '0.0001' '0.0010633140336722136']\n",
      " ['CESM2_' '16' '0.0001' '0.0003612596483435482']\n",
      " ['CNRM_CM6_1_' '14' '0.0001' '0.0001436112361261621']\n",
      " ['CanESM5_' '24' '0.0001' '0.00037265708670020103']\n",
      " ['GISS_E2_1_G_' '18' '0.0001' '0.0002152618981199339']\n",
      " ['IPSL_CM6A_LR_' '16' '0.0001' '0.0004280597495380789']\n",
      " ['MIROC_ES2L_' '14' '0.0001' '0.00012161519407527521']\n",
      " ['MIROC6_' '10' '0.0001' '0.0001678319531492889']\n",
      " ['NorCPM1_' '4' '0.0001' '0.00013684584700968117']\n",
      " ['UKESM1_0_LL_' '18' '0.0001' '0.0004776577407028526']\n",
      " ['GISS_E2_1_H_' '32' '0.0001' '0.00031077489256858826']\n",
      " ['INM_CM5_0_' '18' '0.0001' '6.888752977829427e-05']\n",
      " ['MPI_ESM1_2_HR_' '24' '0.0001' '0.00021580886095762253']\n",
      " ['MPI_ESM1_2_LR_' '22' '0.0001' '0.00010661045962478966']]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "results = np.load('/home/disk/p/aodhan/SurfaceTrendLearing/TropicalTASApplication/NLPLS_1Layer_6components.npy')\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.094972067039107\n"
     ]
    }
   ],
   "source": [
    "weighted_sum_neural = 6*40+16*50+14*29+24*40+18*12+32*16+30*14+50*10+30*4+15*18+10*32+10*18+10*24+10*22\n",
    "total_models = 40+50+29+40+12+32+30+50+30+15+10+10+10\n",
    "print(weighted_sum_neural/total_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "af835c7fa3f1d66f76e7463e82810696e6cb13c598b94db10ac6143493e4a761"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
